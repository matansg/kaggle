{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt, matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "labeled_images = pd.read_csv('train.csv')\n",
    "images = labeled_images.iloc[:,1:]\n",
    "labels = labeled_images.iloc[:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#non linear function and der\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_dev(x):\n",
    "    return (x>0)*1\n",
    "\n",
    "def softmax(x):\n",
    "    A = np.exp(x)\n",
    "    s = np.sum(A, axis = 1).reshape((A.shape[0], 1))\n",
    "    A = (A / s) + 1e-15\n",
    "    return A\n",
    "\n",
    "def softmax_dev(x):\n",
    "    return (1-x)*x\n",
    "\n",
    "def log_loss(A,Y):\n",
    "     return (-np.trace(np.dot(Y, np.log(A).T)) / Y.shape[0])\n",
    "    \n",
    "def log_loss_dev(A,Y):\n",
    "    return (-(Y/A)+(1-Y)/(1-A))\n",
    "#-----------------------start here:--------------------------\n",
    "def create_matrix(X, num_layers):\n",
    "#This function caculation A_out with linear,non linear function\n",
    "    W_all=[]\n",
    "    b_all=[]\n",
    "    \n",
    "    W = np.random.rand(X.shape[1],num_layers[0])*0.01\n",
    "    b = np.random.rand(1,num_layers[0])*0.01\n",
    "    W_all.append(W)\n",
    "    b_all.append(b)\n",
    "    for i in range(np.size(num_layers)-1):\n",
    "        W = np.random.rand(num_layers[i],num_layers[i+1])*0.01\n",
    "        b = np.random.rand(1,num_layers[i+1])*0.01\n",
    "        W_all.append(W)\n",
    "        b_all.append(b)\n",
    "    \n",
    "    \n",
    "    return W_all,b_all\n",
    "\n",
    "    \n",
    "def forward(A_past, W, b, catches, activ_func):\n",
    "#This function caculation A_out with linear,non linear function\n",
    "    Z = np.dot(A_past,W) + b\n",
    "    catches.append(A_past)\n",
    "    catches.append(Z)\n",
    "    if activ_func==\"relu\" :\n",
    "        A=relu(Z)\n",
    "        \n",
    "    if activ_func==\"softmax\" :  \n",
    "        A = softmax(Z)\n",
    "    return A,catches\n",
    "\n",
    "def backward(da_forward, W, b, catches,Layer,active_func):\n",
    "   \n",
    "\n",
    "    A_past=catches[2*Layer]\n",
    "    m=A_past.shape[0]\n",
    "    Z_L=catches[2*Layer+1]\n",
    "    if active_func==\"relu\" :\n",
    "        dev_func=relu_dev(Z_L)\n",
    "    if active_func==\"softmax\" : \n",
    "        dev_func=softmax_dev(softmax(Z_L))\n",
    "        \n",
    "            \n",
    "    dZ = da_forward*dev_func\n",
    "    \n",
    "    dW = np.dot(A_past.T,dZ)/m\n",
    "    db = np.sum(dZ, axis = 0) /m\n",
    "    \n",
    "    dA_now=np.dot(dZ,W.T)\n",
    "    \n",
    "    return dW, db, dA_now\n",
    "\n",
    "def forward_propogation(A_prev, W, b, type_layers):\n",
    "    catches=[]\n",
    "\n",
    "    for i in range(np.size(type_layers)):\n",
    "        W_now=W[i]\n",
    "        b_now=b[i]\n",
    "        [A_prev,catches] = forward(A_prev, W_now, b_now,catches,type_layers[i]) \n",
    "    return A_prev,catches\n",
    "\n",
    "def backword_propogation(Y, A, W, b, type_layers,loss_func,catches,learning_rate):\n",
    "    dW_all=[]\n",
    "    db_all=[]\n",
    "    if loss_func==\"Logloss\":\n",
    "        da=log_loss_dev(A,Y)   \n",
    "    for i in range(np.size(type_layers),0,-1):\n",
    "        [dW, db, da]=backward(da, W[i-1], b[i-1],catches,i-1,type_layers[i-1])\n",
    "        #print(np.sum(dz-(A-Y)))\n",
    "        W[i-1]=W[i-1]-learning_rate*dW\n",
    "        b[i-1]=b[i-1]-learning_rate*db\n",
    "    \n",
    "    return W,b\n",
    "\n",
    "def update(X,Y, W, b, type_layers,A,loss_func,catches,learning_rate):\n",
    "    [A,catches]=forward_propogation(X, W, b, type_layers)\n",
    "    J=log_loss(A,Y)\n",
    "    [W,b]=backword_propogation(Y, A, W, b, type_layers,loss_func,catches,learning_rate)\n",
    "    \n",
    "    return W, b, J\n",
    "\n",
    "\n",
    "def predict(X, W, b, type_layers):\n",
    "    A = forward_propogation(X, W, b, type_layers)\n",
    "    return np.argmax(A, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-318-31e196d8e2ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         [W, b, J]=update(X[order[i : i + minibatch_size],:],Y[order[i : i + minibatch_size],:],\\\n\u001b[1;32m---> 24\u001b[1;33m                          W, b, type_layers,A,loss_func,catches,learning_rate)\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mJ_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-310-12f5ad1ac12a>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(X, Y, W, b, type_layers, A, loss_func, catches, learning_rate)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcatches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[1;33m[\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcatches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforward_propogation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0mJ\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackword_propogation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype_layers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcatches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-310-12f5ad1ac12a>\u001b[0m in \u001b[0;36mforward_propogation\u001b[1;34m(A_prev, W, b, type_layers)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mW_now\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mb_now\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcatches\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_now\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_now\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcatches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtype_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "m = 2000\n",
    "learning_rate = 0.5\n",
    "lambd = 0\n",
    "minibatch_size = 1024\n",
    "iters = 1000\n",
    "\n",
    "X = images.values[:m, :] / 256.\n",
    "Y = np.zeros((m, 10), dtype = np.int32)\n",
    "for i in range(m):\n",
    "    Y[i, labels.values[i, :]] = 1\n",
    "\n",
    "\n",
    "loss_func=\"Logloss\"    \n",
    "number_layers=[300,10]\n",
    "type_layers=[\"relu\",\"relu\",\"softmax\"]\n",
    "[W,b]=create_matrix(X, number_layers)\n",
    "J_all=[]\n",
    "for epoch in range(iters):\n",
    "    order = np.random.permutation(m)\n",
    "    epoch_score = 0\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        [W, b, J]=update(X[order[i : i + minibatch_size],:],Y[order[i : i + minibatch_size],:],\\\n",
    "                         W, b, type_layers,A,loss_func,catches,learning_rate)\n",
    "        J_all.append(J)\n",
    "    \n",
    "plt.plot(J_all)\n",
    "[A,catches]=forward_propogation(X, W, b, type_layers)\n",
    "print(np.sum(np.argmax(A, axis = 1) == labels.values[:m, :].reshape((m, ))) / m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89335\n"
     ]
    }
   ],
   "source": [
    "X = images.values[m:, :] / 256.\n",
    "m_new=X.shape[0]\n",
    "[A,catches]=forward_propogation(X, W, b, type_layers)\n",
    "print(np.sum(np.argmax(A, axis = 1) == labels.values[m:, :].reshape((m_new, ))) / m_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(m_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
